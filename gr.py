# Ollama ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸

# langchain_community íŒ¨í‚¤ì§€ì—ì„œ ChatOllama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from langchain_community.chat_models import ChatOllama
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# gradio íŒ¨í‚¤ì§€ì—ì„œ gr ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
import gradio as gr

# ChatOllama ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ model ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
# "gemma:2b-instruct" ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
model = ChatOllama(model="gemma:2b-instruct", temperature=0)

def response(message, history, additional_input_info):
        history_langchain_format = []
        # additional_input_infoë¡œ ë°›ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë­ì²´ì¸ì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ì— í¬í•¨ì‹œí‚¨ë‹¤.
        history_langchain_format.append(SystemMessage(content= additional_input_info))
        for human, ai in history:
                history_langchain_format.append(HumanMessage(content=human))
                history_langchain_format.append(AIMessage(content=ai))
        history_langchain_format.append(HumanMessage(content=message))
        gpt_response = model(history_langchain_format)
        return gpt_response.content

gr.ChatInterface(
        fn=response,
        textbox=gr.Textbox(placeholder="ì§ˆë¬¸í•´ì£¼ì„¸ìš”", container=False, scale=7),
        # ì±„íŒ…ì°½ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•œë‹¤.
        chatbot=gr.Chatbot(height=1000),
        title="ì–´ë–¤ ì±—ë´‡ì„ ì›í•˜ì‹¬ë¯¸ê¹Œ?",
        description="ë¬¼ì–´ë³´ë©´ ë‹µí•˜ëŠ” ì±—ë´‡ì„ë¯¸ë‹¤.",
        theme="soft",
        retry_btn="ë‹¤ì‹œë³´ë‚´ê¸° â†©",
        undo_btn="ì´ì „ì±— ì‚­ì œ âŒ",
        clear_btn="ì „ì±— ì‚­ì œ ğŸ’«",
        additional_inputs=[
            gr.Textbox("", label="System Promptë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”", placeholder="")
        ]
).launch()